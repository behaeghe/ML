---
title: "How well can a bunch of people handle dumbells... Machine Learning to the rescue"
output:
  html_document: default
  html_notebook: default
---
#Introduction
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.
#Getting the data 
The data set of interest has been generously provided by groupware@LES. The data set used here is from their original work published
under:

*Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.
Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4Qrl1EXzu*

We will load the data provided as csv files.  

```{r loading the raw data,tidy=TRUE,echo=FALSE}
##Loading the data
trainURL <-  "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train <- read.csv(trainURL,stringsAsFactors = FALSE)
test <- read.csv(testURL,stringsAsFactors = FALSE)
```
#Cleaning the data set
A quick review of the raw data set shows a lot of variables with little or no data ("NAs"), in order to be able to run our model definition and selection appropriately we will discard variables that have more than 50% of their data missing. We will also eliminate the extraneous variables related to the data collection (timestamps, users etc..). This helps us reduce the number of possible predictors from 159 variables to 53. This is before we perform any feature selection.  


```{r cleaning our data set, echo=TRUE, tidy=TRUE,cache=TRUE}
require(tidyverse)
## checking for null values or lack of data
train.countna <- as.data.frame(apply(sapply(train,is.na),2,sum),stringsAsFactors=FALSE)# Checking for missing values
tmprow <- row.names(train.countna) #Variables name are the row names of the data frame so, need to extract those and make them a variable
train.countna <- cbind(tmprow,train.countna) #binding the variable names as a variable in the data frame
colnames(train.countna) <- c("varname","count") #cleaning up the data frame with some sensible variables name
train.include <- as.character(train.countna[1-(train.countna$count/length(train$X)) >0.5,]$varname) # we only want variables that are not missing more than 50% of their data
train.filter <- dplyr::select(train,one_of(train.include)) # let's filter on those
## after this we still have some sparse columns that may not be of interest
predictors <- 8:length(colnames(train.filter))-1 ## Identififying our predictors vectors
tmp <- train.filter[,predictors] #focusing on predictors
mycols <- data.frame(names(lapply(tmp,class)),unlist(lapply(tmp,class))) 
colnames(mycols) <- c("varname","class")
predictors <-  as.character(filter(mycols,class !="character")[[1]]) #Extracting the predictors of class character that 
#have no or little data
nonpredictors <- colnames(train.filter[,c(1:7,length(colnames(train.filter)))])
collist <- c(nonpredictors,predictors) #Re-assembling our data frame
train.model <- select(train.filter,one_of(collist)) #subseting to the predictor vectors that contains actual data
train.model <- train.model[,-(1:7)] # This is the clean dataset we want to work on
train.model$classe <- as.factor(train.model$classe)
#subset our test data accrordingly
test.model <- select(test,one_of(collist))
test.model <- test.model[,-(1:7)]

##Now cleanup the environment and ony keep, test.model and train.model
rm(list=ls()[!ls() %in% c("train","test","train.model","test.model")])
```
```{r environmnet clean up}
cleanup <- ls()
rm(list=cleanup[cleanup != "test.model" & cleanup != "train.model"])
rm(list=c("cleanup"))
```
#Feature Selection  

The goal of feature selection is to reduce the number of predictors to improve training quality of our models and reduce compute time during training.  

## Higly correlated variables  


We are using the feature selection tool set from the caret package, notably ```findCorrelation``` to identify paired of highly correlated (over correlation over .75 in our case) variables and eliminate one of the paired vairable from the predictors. 

```{r feature selection, tidy=TRUE}
set.seed(1234)
require(caret)
#calculate te correlation matrix
corrMat <- cor(train.model[,-1])
#find highly correlated variables over 0.75 correlation
corrVar <- findCorrelation(corrMat,cutoff=0.75)
#reduce model by eliminating correlated variables, shift by 1 as we added eliminated the classe variable early
train.model <- train.model[,-(corrVar+1)]
```
Our intent is to use ensemble learning methods that provides additional automated feature selection. There's no need to do additional feature selection in this case.  

# Our model 

Since we need to predict a factor variable, we would look at a classification method. Random Forests offer many advantages, one of them is that they are simpler to train and tune that other methods.Since they offer similar performance as bagging techniques at reducing variance, yet are simpler to refine, we will start by fitting a random forests. Based on the accuracy achieved we may decide to fit additional model.  

# Training and evaluation  

We will use a k-fold cross validation method, with a value of 10 for k. The value of 10 has been suggested by the litterature ("The Elements of Statistical Learning").  
We are partitioning our data set in a training and validation set, with a 70/30 ratio. We will traing a Random Forest using the caret default for paramenets: mtry will be $\sqrt{n}$ to start with and we will go 500 trees deep.
``` {r model training and selection}

library(knitr)
library(pander)
mytrain <- train.model
##creating a train and validation set
set.seed(125)
inTrain <- createDataPartition(mytrain$classe,p=0.6,list=FALSE)
trainset <- mytrain[inTrain,]
valset <- mytrain[-inTrain,]
## We want to do a cross validation over 10 folds, so creating the cv for the caret::train method
train_control <- trainControl(method="cv",number=10,savePredictions=TRUE)
set.seed(125)
# fitting our random forest
fit <- train(classe ~ ., data=trainset,method="rf",trControl=train_control)
# predicting on our validation set to assess OOS accuracy
predict <- predict(fit,valset)
saveRDS(fit,file="rfCaretUntuned.dat") #Saving the model (as it is compute intensive)
confMatrix.rf <- confusionMatrix(predict,valset$classe)
## our out of sample erros is the number of missed prediction divided by the number of total cases
oos.error <- 1- sum(predict==valset$classe)/nrow(valset)
##Accuracy of our model by # of predictors
##evaluating the quizz results while we are at it
quizz_result.caret <- predict(fit,test.model)
end <- Sys.time()
```
#Considerations for our model  

Well, it wasn't that bad and we will stick to it. Our accuracy in ```r round(fit$results$Accuracy[1],3) ```. The following plot illustrates our model accuracy as a function of the number of predictors (mtry).  

```{r plot}
 plot(fit, log = "y", lwd = 2, main = "Random forest accuracy", xlab = "# of Predictors (mtry)", 
    ylab = "Accuracy")
```
```{r confusion matrix, echo=FALSE,results='asis'}
pander(confMatrix.rf$byClass,caption="Confusion Matrix on Validation Dataset")
```
We have calculated our OOS error rate by summing all the misses and dividing by the total number of cases assessed. We evalute our out of sample error rate at : ```r 100* round(oos.error,3)```%. Not bad for a first try !  
Here are our predictions using our model on the 20 test cases:  



#Conclusion  
A simple random forests model with the appropriate feature selection delivered a reasonably accurate method on our first try !

 